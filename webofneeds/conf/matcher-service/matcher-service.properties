#
# Copyright 2012  Research Studios Austria Forschungsges.m.b.H.
#
#    Licensed under the Apache License, Version 2.0 (the "License");
#    you may not use this file except in compliance with the License.
#    You may obtain a copy of the License at
#
#        http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS,
#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#    See the License for the specific language governing permissions and
#    limitations under the License.
#

# sparql endpoint of rdfstore used
uri.sparql.endpoint=http://localhost:9999/blazegraph/namespace/kb/sparql

# won nodes to crawl/skip + connect to for atom updates and hints, default are the localhost node (for development purpose)
# and the node on matchat.org so that new webofneeds instances by default connect to the main matchat instance
# if you overwrite this property with your own won nodes to crawl, consider adding "https://node.matchat.org/won/resource"
# so that your matcher connects to all atoms available in the publicly known atoms on matchat.org
wonNodeController.wonNode.crawl=https://localhost:8443/won/resource,https://node.matchat.org/won/resource
wonNodeController.wonNode.skip=
wonNodeController.wonNode.lifeCheckDuration=60000

# crawler properties
# This class uses property paths to extract URIs from linked data resources. These property paths are executed
# relative to base URIs. Therefore there are two types of property paths. Base property path extract URIs that are
# taken as new base URIs. Non-base property paths extract URIs that keep the current base URI.
crawler.propertyPaths.base=(<http://www.w3.org/2000/01/rdf-schema#member>|<https://w3id.org/won/core#connections>|<https://w3id.org/won/core#targetAtom>|<https://w3id.org/won/core#sourceAtom>|<https://w3id.org/won/core#holds>|<https://w3id.org/won/core#heldBy>)
crawler.propertyPaths.nonBase=

# time in minutes until won nodes are crawled the next time
crawler.recrawl.interval.minutes=10

# execute the meta data rdf update at least every x milliseconds
crawler.metaDataUpdate.maxDuration=10000

# update number of meta data  crawling messages at once by bulk update
crawler.metaDataUpdate.maxBulkSize=10

# http connection and read timeouts for crawler
crawler.http.timeout.connection=2000
crawler.http.timeout.read=2000



# default won node uri property needs to be available for spring initialization but can be left empty
uri.node.default=

#should identify this matcher and be a 'cool URI' providing metadata about the matcher
matcher.uri=https://localhost:8443/matcher
#matcher.uri=http://sat001.researchstudio.at:8080/matcher

# keystore temporary properties (password is now not used from here but hardcoded - that should be changed)
keystore.password=temp
keystore.location=/usr/src/matcher-service/client-certs/matcher-keys.jks

# truststore properties (password is now not used from here but hardcoded - that should be changed)
truststore.password=temp
truststore.location=/usr/src/matcher-service/client-certs/matcher-trusted-certs.jks


# turn this on if you want to monitor the system
matcher.service.monitoring=false

# directory where monitoring statistics are periodically dumped (leave empty to let java create temp files)
monitoring.output.dir=

# milliseconds between two consecutive monitoring statistics dumps
monitoring.interval.seconds=3600

# reset the monitoring stats after output? (allows for comparing periods to track change more clearly)
monitoring.reset.after.output=true
