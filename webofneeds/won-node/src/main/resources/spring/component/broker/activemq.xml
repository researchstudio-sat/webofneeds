<!--
    Licensed to the Apache Software Foundation (ASF) under one or more
    contributor license agreements.  See the NOTICE file distributed with
    this work for additional information regarding copyright ownership.
    The ASF licenses this file to You under the Apache License, Version 2.0
    (the "License"); you may not use this file except in compliance with
    the License.  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
-->
<!-- START SNIPPET: example -->

<spring:beans
        xmlns="http://activemq.apache.org/schema/core"
        xmlns:spring="http://www.springframework.org/schema/beans"
        xmlns:context="http://www.springframework.org/schema/context"
        xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
        xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.1.xsd
  http://activemq.apache.org/schema/core http://activemq.apache.org/schema/core/activemq-core.xsd
  http://www.springframework.org/schema/context
  http://www.springframework.org/schema/context/spring-context-4.1.xsd">
    <!--
        The <broker> element is used to configure the ActiveMQ broker.
    -->

    <!-- This bean extends the the abstract PPC bean in the parent context -->
    <spring:bean id="propertySourcesPlaceholderConfigurer" parent="abstractPropertySourcesPlaceholderConfigurer" />

    <broker xmlns="http://activemq.apache.org/schema/core" brokerName="wonBroker" persistent="false" >

        <destinationPolicy>
            <policyMap>
              <policyEntries>
              	<!-- producer flow control causes congestion in our setting. disable and discard pending messages -->
              	<!-- decided after reviewing https://access.redhat.com/documentation/en-US/Fuse_ESB_Enterprise/7.1/html-single/ActiveMQ_Tuning_Guide/index.html -->
              	<policyEntry topic=">" producerFlowControl="false" memoryLimit="5mb" optimizedDispatch="true" queuePrefetch="1">
              		<pendingMessageLimitStrategy>
                    	<constantPendingMessageLimitStrategy limit="10"/>
                  	</pendingMessageLimitStrategy>
              	</policyEntry>
              	<policyEntry queue=">" producerFlowControl="false" memoryLimit="5mb" optimizedDispatch="true" queuePrefetch="1">
              		<pendingMessageLimitStrategy>
                    	<constantPendingMessageLimitStrategy limit="10"/> 
                  	</pendingMessageLimitStrategy>
              	</policyEntry>
                	<policyEntry queue=">">
						<!-- 
				           Tell the dead letter strategy not to process expired messages
				           so that they will just be discarded instead of being sent to
				           the DLQ 
				         -->
				         <deadLetterStrategy>
							<sharedDeadLetterStrategy processExpired="false" />
				         </deadLetterStrategy>
		       		</policyEntry>
            	</policyEntries>
            </policyMap>
        </destinationPolicy>


        <!--
            The managementContext is used to configure how ActiveMQ is exposed in
            JMX. By default, ActiveMQ uses the MBean server that is started by
            the JVM. For more information, see:

            http://activemq.apache.org/jmx.html
        -->
        <managementContext>
            <managementContext createConnector="false"/>
        </managementContext>

        <!--
            Configure message persistence for the broker. The default persistence
            mechanism is the KahaDB store (identified by the kahaDB tag).
            For more information, see:

            http://activemq.apache.org/persistence.html
        -->
        <!--persistenceAdapter>
            <kahaDB directory="dataDirectory"/>
        </persistenceAdapter-->


          <!--
            The systemUsage controls the maximum amount of space the broker will
            use before slowing down producers. For more information, see:
            http://activemq.apache.org/producer-flow-control.html
            If using ActiveMQ embedded - the following limits could safely be used:

        <systemUsage>
            <systemUsage>
                <memoryUsage>
                    <memoryUsage limit="20 mb"/>
                </memoryUsage>
                <storeUsage>
                    <storeUsage limit="1 gb"/>
                </storeUsage>
                <tempUsage>
                    <tempUsage limit="100 mb"/>
                </tempUsage>
            </systemUsage>
        </systemUsage>
        -->
          <systemUsage>
          <!--sendFailIfNoSpace="true" or sendFailIfNoSpaceAfterTimeout results in ResourceAllocationException if no space on
          broker is available, we can react to this exception on Client side, e.g. an owner can connect to another node -->
            <systemUsage sendFailIfNoSpace="true">
                <memoryUsage>
                    <!-- overall memory for messages on all queues/topics -->
                    <memoryUsage limit="20 mb"/>
                </memoryUsage>
                <storeUsage>
                    <storeUsage limit="200 mb"/>
                </storeUsage>
                <tempUsage>
                    <tempUsage limit="100 mb"/>
                </tempUsage>
            </systemUsage>
        </systemUsage>

        <!--
            The transport connectors expose ActiveMQ over a given protocol to
            clients and other brokers. For more information, see:

            http://activemq.apache.org/configuring-transports.html
        -->
        <transportConnectors>
                <!--transportConnector name="websocket" uri="ws://0.0.0.0:61614"/-->
            <!-- DOS protection, limit concurrent connections to 1000 and frame size to 100MB -->
            <!--transportConnector name="openwire" uri="tcp://localhost:62616?maximumConnections=1000&amp;wireformat.maxFrameSize=104857600"/-->
            <!--transportConnector name="amqp" uri="amqp://localhost:5672?maximumConnections=1000&amp;wireformat.maxFrameSize=104857600"/-->
            <!--transportConnector name="tcp"
            uri="${uri.protocol.activemq}?maximumConnections=1000&amp;wireformat.maxFrameSize=104857600" /-->
            <!--transportConnector name="ssl"
                                uri="${uri.protocol.activemq}?maximumConnections=1000&amp;wireformat.maxFrameSize=104857600&amp;needClientAuth=true&amp;trace=true" /-->

	            <transportConnector name="${activemq.broker.scheme}"
                                uri="${activemq.broker.scheme}://0.0.0.0:${activemq.broker.port}?maximumConnections=1000&amp;wireformat.maxFrameSize=104857600&amp;needClientAuth=true&amp;jms.useAsyncSend=true&amp;transport.useInactivityMonitor=false&amp;wireFormat.maxInactivityDuration=0&amp;maximumRedeliveries=0" />
 
            <!--transportConnector name="ssl"
                                uri="${uri.protocol.activemq}?maximumConnections=1000&amp;wireformat.maxFrameSize=104857600&amp;needClientAuth=true&amp;transport.enabledProtocols=TLSv1.2" /-->
            <!-- &amp;transport.enabledProtocols=TLSv1.2-->
            <transportConnector name="vm" uri="vm://localhost?marshal=false&amp;broker.persistent=false&amp;jms.useAsyncSend=true&amp;wireFormat.maxInactivityDuration=0&amp;transport.useInactivityMonitor=false&amp;maximumRedeliveries=0" />

        </transportConnectors>
        <sslContext>
        	<!-- This is actually not tricky.
        	
        	The activeMQ broker runs in a child context of the won-node-webapp. Both the webapp and the 
        	activeMQ broker requires TLS clients to provide a client certificate, but actually do not 
        	check it upon connection request. The webapp and activemq thus must impersonate the same entity 
        	externally. However, if behind nginx (as in the case of a live deployment), they read in their private
        	key in different formats (activemq: jks, nginx: pem) - that has been a source of trouble in the
        	past, as sometimes these files get out of sync and don't contain the same key. 
        	
        	Before making changes to this config, see https://github.com/researchstudio-sat/webofneeds/issues/891
        	        	
        	If establishing a TLS connection fails, it is because the client cannot verify the
        	server's certificate. It has nothing to do with the server's trust store. It is ok to use
        	the AcceptAllCertsTrustManager here.  
        	
        	Specifically, it is not necessary (and probably causes a slow-down when many keys are trusted, to use a TrustManager 
        	(nodeTrustManagerTLS) that uses the TrustStoreService also accessed by the node webapp.
        	-->
            <sslContext
	             keyStore="${activemq.broker.keystore}"
	             keyStorePassword="${activemq.broker.keystore.password}"
	             >
	             <trustManagers>
                        <spring:bean class="won.utils.tls.AcceptAllCertsTrustManager" />
                 </trustManagers>
            </sslContext>
        </sslContext>

        <plugins>
            <spring:bean id="certificateCheckingPlugin" class="won.cryptography.activemq.CertificateCheckingBrokerPlugin">
                <spring:property name="queueNamePrefixToCheck" value="OwnerProtocol.Out." />
            </spring:bean>
        </plugins>

        <!-- destroy the spring context on shutdown to stop jetty -->
        <shutdownHooks>
            <bean xmlns="http://www.springframework.org/schema/beans" class="org.apache.activemq.hooks.SpringContextHook" />
        </shutdownHooks>

    </broker>

    <!--
        Enable web consoles, REST and Ajax APIs and demos

        Take a look at ${ACTIVEMQ_HOME}/conf/jetty.xml for more details
    -->
    <!-- <import resource="jetty.xml"/>       -->

</spring:beans>
<!-- END SNIPPET: example -->